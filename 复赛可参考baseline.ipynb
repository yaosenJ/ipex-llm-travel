{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "恭喜你一路来到复赛！如今你已经是 IPEX-LLM 工具链的使用专家了；得益于 Intel 工具链对大语言模型推理的优化，如今我们可以让 LLM 应用在 Intel CPU 上获得强大的推理性能。在本次复赛中，我们将使用性能更加强大的 g8i 实例，我们将在搭载了英特尔最新第四代英特尔® 至强®可扩展处理器上运行更大参数量的大语言模型甚至扩散模型、多模态模型。\n",
    "你是不是在初赛环节觉得有些意犹未尽？让我们在复赛环节真正的大展身手，共同感受 CPU 推理的极致性能体验。\n",
    "\n",
    "首先，我们需要前往阿里云进入服务器界面，我们需要依次选择云服务器ECS——立即购买——按量付费；\n",
    "随后，你需要完成以下操作：\n",
    "1. 在第3页找到通用型g8i `ecs.g8i.6xlarge  24vCPU` 对应型号，镜像选择 Ubuntu 22.04 64位\n",
    "2. 找到带宽和安全组，点选公网IP中的 `分配公网IPv4地址`\n",
    "3. 选择`自定义密码`并设置密码后即可确认下单（价格约一小时6.321元）\n",
    "\n",
    "至此，服务器实例创建完毕，你也拥有了一个可以公网访问的IP地址。（记得密码要写的复杂！否则容易被攻击，建议大小写特殊符号都要有）\n",
    "\n",
    "> **❗ 重要信息**：如果您不会持续使用相关服务器实例，可以考虑下列方案,在保存现有代码和模型等数据的同事，节省费用支出：\n",
    ">\n",
    "> 请在实例界面选择停止实例后选择 **节省停机模式** ，待下次进入时可正常恢复开发环境，同时节约计费；如果你想完全停止所有实例计费，你需要在`更多操作`中完全释放实例，若仍是担心费用问题，可在左侧的 **块存储（云盘）** 处检查硬盘资源是否成功释放。\n",
    "\n",
    "\n",
    "注意，在创建实例后，推荐在正式进入服务器之前，先进行存储的扩容。我们可以点击左侧`存储与快照`下的`块存储（云盘）`，看到当前有个使用中的系统盘，随后找到操作中的`扩容`，将其扩充至200~400G即可。\n",
    "\n",
    "请参加比赛的各团队伙伴根据项目实际带宽及流量需求，综合考虑后选择合适的流量/带宽计费方案，可以根据实际情况需求进行动态修改。\n",
    "\n",
    "\n",
    "# 一、安装环境\n",
    "\n",
    "创建实例后，点击远程连接即可进入机器，但此时不方便操作，我们可以通过 vscode ssh 远程连接到服务器实例，通过密码验证直接登录 `ssh root@xxx.xxx.xxx.xxx`。进入机器后，我们需要安装最新的 IPEX-LLM 程序，你也可以直接把这个notebook移动到服务器上 `/home` 目录下进行操作。vscode需要安装python和jupyter依赖等。\n",
    "\n",
    "> 我们并不需要拘泥于当前的 IPEX-LLM 技术方案，欢迎大家使用各种比赛规则中推荐的部署方案进行项目实现。    \n",
    ">\n",
    "> 本notebook仅提供一种部署及实现RAG的参考方案，更多方案、以及资料请参考：   \n",
    "> - [OpenVINO LLMs](https://docs.llamaindex.ai/en/stable/examples/llm/openvino/)  \n",
    "> - [llm-rag-langchain-with-output](https://docs.openvino.ai/nightly/notebooks/llm-rag-langchain-with-output.html)\n",
    "> - [llm-rag-llamaindex-with-output](https://docs.openvino.ai/nightly/notebooks/llm-rag-llamaindex-with-output.html)\n",
    "> - [Intel® Extension for Transformers](https://github.com/intel/intel-extension-for-transformers/blob/main/docs/weightonlyquant.md#examples-for-gpu)\n",
    "> - [xFasterTransformer](https://github.com/intel/xFasterTransformer)\n",
    "> - [Intel Extension for Pytorch](https://github.com/intel/intel-extension-for-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install modelscope\n",
    "!pip install ipex-llm==2.1.0b20240805\n",
    "!pip install transformers==4.37.0 accelerate\n",
    "!pip install PyMuPDF llama-index-vector-stores-chroma llama-index-readers-file llama-index-embeddings-huggingface llama-index\n",
    "!pip install py-cpuinfo\n",
    "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、模型准备\n",
    "\n",
    "Qwen2是阿里云最新推出的开源大型语言模型系列，相比Qwen1.5，Qwen2实现了整体性能的代际飞跃，大幅提升了代码、数学、推理、指令遵循、多语言理解等能力。\n",
    "\n",
    "包含5个尺寸的预训练和指令微调模型：Qwen2-0.5B、Qwen2-1.5B、Qwen2-7B、Qwen2-57B-A14B和Qwen2-72B，其中Qwen2-57B-A14B为混合专家模型（MoE）。所有尺寸模型都使用了GQA（分组查询注意力）机制，以便让用户体验到GQA带来的推理加速和显存占用降低的优势。\n",
    "\n",
    "在中文、英语的基础上，训练数据中增加了27种语言相关的高质量数据。增大了上下文长度支持，最高达到128K tokens（Qwen2-72B-Instruct）。\n",
    "\n",
    "在这里，我们将使用 `Qwen/Qwen2-7B-Instruct` 的模型参数版本来体验 Qwen2 的强大能力。\n",
    "\n",
    "首先，我们需要对模型进行下载，我们可以通过 modelscope 的 api 很容易实现模型的下载：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "import os\n",
    "# 第一个参数表示下载模型的型号，第二个参数是下载后存放的缓存地址，第三个表示版本号，默认 master\n",
    "model_dir = snapshot_download('Qwen/Qwen2-7B-Instruct', cache_dir='/home/qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载完成后，我们将对 qwen2 模型进行低精度量化至 int4 ，低精度量化（Low Precision Quantization）是指将浮点数转换为低位宽的整数（这里是int4），以减少计算资源的需求和提高系统的效率。这种技术在深度学习模型中尤其重要，它可以在硬件上实现快速、低功耗的推理，也可以加快模型加载的速度。\n",
    "\n",
    "经过 Intel ipex-llm 优化后的大模型加载 api `from ipex_llm.transformers import AutoModelForCausalLM`， 我们可以很容易通过 `load_in_low_bit='sym_int4'` 将模型量化到 int4 ，英特尔 IPEX-LLM 支持 ‘sym_int4’, ‘asym_int4’, ‘sym_int5’, ‘asym_int5’ 或 'sym_int8’选项，其中 ‘sym’ 和 ‘asym’ 用于区分对称量化与非对称量化。 最后，我们将使用 `save_low_bit` api 将转换后的模型权重保存到指定文件夹。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "from transformers import  AutoTokenizer\n",
    "import os\n",
    "if __name__ == '__main__':\n",
    "    model_path = os.path.join(os.getcwd(),\"/home/qwen2chat_src/Qwen/Qwen2-7B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, load_in_low_bit='sym_int4', trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model.save_low_bit('/home/qwen2chat_int4')\n",
    "    tokenizer.save_pretrained('/home/qwen2chat_int4')\n",
    "    print(\"保存完毕！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备完转换后的量化权重，接下来我们将在终端中第一次运行 qwen2 在 CPU 上的大模型推理，但请注意不要在 notebook 中运行（本地运行可以在 notebook 中运行，由于魔搭 notebook 和终端运行脚本有一些区别，这里推荐在终端中运行。\n",
    "\n",
    "在运行下列代码块后，将会自动在终端中新建一个python文件，我们只需要在终端运行这个python文件即可启动推理：\n",
    "\n",
    "```python\n",
    "cd /home\n",
    "python3 run_stream.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/run_stream.py\n",
    "# 设置OpenMP线程数为8\n",
    "import os\n",
    "\n",
    "import time\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# 导入Intel扩展的Transformers模型\n",
    "from ipex_llm.transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 加载模型路径\n",
    "load_path = \"qwen2chat_int4\"\n",
    "\n",
    "# 加载4位量化的模型\n",
    "model = AutoModelForCausalLM.load_low_bit(load_path, trust_remote_code=True)\n",
    "\n",
    "# 加载对应的tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_path, trust_remote_code=True)\n",
    "\n",
    "# 创建文本流式输出器\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "# 设置提示词\n",
    "prompt = \"给我讲一个芯片制造的流程\"\n",
    "\n",
    "# 构建消息列表\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "# 使用推理模式\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # 应用聊天模板,添加生成提示\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # 对输入文本进行编码\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "    \n",
    "    print(\"start generate\")\n",
    "    st = time.time()  # 记录开始时间\n",
    "    \n",
    "    # 生成文本\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        max_new_tokens=1024,  # 最大生成512个新token\n",
    "        streamer=streamer,   # 使用流式输出\n",
    "    )\n",
    "    \n",
    "    end = time.time()  # 记录结束时间\n",
    "    \n",
    "    # 打印推理时间\n",
    "    print(f'Inference time: {end-st} s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、在 g8i 上实现 RAG 模块\n",
    "\n",
    "在之前教程中,我们已经学会了如何应用 IPEX-LLM 加速推理,也学会了如何快速搭建一个基于 IPEX-LLM 的 RAG 系统.\n",
    "\n",
    "让我们再来复习一下。LlamaIndex主要包括以下几个组件:\n",
    "- 数据连接器：帮助连接现有数据源和数据格式（如API、PDF等），并将这些数据转换为LlamaIndex可用的格式。\n",
    "- 数据索引：帮助结构化数据以适应不同的用例。加载了来自不同数据源的数据后，如何将它们分割、定义关系和组织，以便无论您想要解决的问题（问答、摘要等），都可以使用索引来检索相关信息。\n",
    "- 查询接口：是输入查询并从LLM中获取知识增强输出的接口。\n",
    "\n",
    "我们可以来看一个简单的 LlamaIndex 示例,它直观展示了如何构建一个 RAG 体系:\n",
    "\n",
    "假设你有如下的文件组织:\n",
    "```\n",
    "├── starter.py\n",
    "└── data\n",
    "    └── paul_graham_essay.txt\n",
    "```\n",
    "\n",
    "核心代码为:\n",
    "```python\n",
    "# 导入需要的模块和类\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# 1. 使用 SimpleDirectoryReader 加载数据\n",
    "# SimpleDirectoryReader 是一个简单的目录读取器，能从指定目录中读取所有文件的数据\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 2. 设置嵌入模型为 bge-base\n",
    "# HuggingFaceEmbedding 是一个嵌入模型类，用于将文本转换为向量表示\n",
    "# 这里我们使用的是 \"BAAI/bge-base-en-v1.5\" 模型\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# 3. 使用 Ollama 快速接入大语言模型\n",
    "# Ollama 是一个模型的快速调用框架\n",
    "# 这里我们指定使用 \"llama3\" 模型，并设置请求超时时间为 360 秒\n",
    "Settings.llm = Ollama(model=\"llama3\", request_timeout=360.0)\n",
    "\n",
    "# 4. 创建一个向量存储索引\n",
    "# VectorStoreIndex 是一个用于存储和查询向量的索引类\n",
    "# from_documents 方法是从文档数据创建索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 5. 将索引转换为查询引擎\n",
    "# as_query_engine 方法将现有的向量存储索引转换为一个查询引擎\n",
    "# 查询引擎用来对存储的数据进行语义查询\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# 6. 使用查询引擎进行查询\n",
    "# query 方法接受一个查询字符串，并返回一个响应对象\n",
    "# 这里我们查询 \"作者小时候做了什么？\"\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "\n",
    "# 7. 打印查询结果\n",
    "# 打印从查询引擎返回的响应\n",
    "print(response)\n",
    "```\n",
    "\n",
    "从代码中,我们可以很容易看到 RAG 系统构建过程,首先我们需要一个读取器来获得某个目录的对应数据,接着需要对这个数据进行 embedding 化即创建索引,把他转为向量表示,最后就可以用设定好的大模型,结合 query 与进行检索增强生成的对话.\n",
    "\n",
    "接下来,我们将基于 Llamaindex 正式构建一个简易的 RAG 系统,首先我们需要下载中文 Embedding 模型并安装 RAG 系统所需的全部依赖,在这里我们将使用 pdf 文件作为示范."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "# 第一个参数表示下载模型的型号，第二个参数是下载后存放的缓存地址，第三个表示版本号，默认 master\n",
    "model_dir = snapshot_download('AI-ModelScope/bge-small-zh-v1.5', cache_dir='/home/qwen2chat_src', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上述推荐的 Embedding 模型以外，我们还可以尝试[更大参数](https://huggingface.co/BAAI/bge-large-zh-v1.5)的 Embedding 模型，BGE 是北京智源人工智能研究院开源的嵌入模型家族，你可以使用但不限于以下几类模型：\n",
    "\n",
    "```\n",
    "BAAI/bge-m3\n",
    "BAAI/bge-large-zh-v1.5\n",
    "BAAI/bge-base-zh-v1.5\n",
    "BAAI/bge-small-zh-v1.5\n",
    "```\n",
    "\n",
    "你可以在[huggingface官网](https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d)寻找到更多讯息，对于不同语言的初始文本，我们也推荐使用不同语言的嵌入模型进行调试，以便最大程度提升向量化的成效。除此之外，我们还推荐使用 [rerank 模型](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_reranker) 进行检索重排操作，这会为检索结果带来进一步的改善，增强问答效果。\n",
    "\n",
    "\n",
    "准备好向量化所需的 Embedding 模型后,我们还需要提前准备 RAG 使用的 pdf 文件进行向量化处理,你可以按照如下文件组织存放对应的 pdf 文件,你可以根据实际情况考虑是否提前构建好向量数据库,此时直接使用 LlamaIndex 再次加载即可使用.\n",
    "\n",
    "```\n",
    "├── run_rag.py\n",
    "└── data\n",
    "    └── 2407.10671v3.pdf\n",
    "```\n",
    "\n",
    "存放完 pdf 后,你需要修改下列 run_rag.py 代码块文件中的 `Config` 配置,将 question 修改成你想要对材料的提问,将 data_path 设定为 pdf 的文件地址,随后可以运行代码块创建出带运行的 RAG python 工程文件,紧接着按照下列方式即可启动:\n",
    "\n",
    "```python\n",
    "cd /home\n",
    "python3 run_rag.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile /home/run_rag.py\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "\n",
    "# 从llama_index库导入HuggingFaceEmbedding类，用于将文本转换为向量表示\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# 从llama_index库导入ChromaVectorStore类，用于高效存储和检索向量数据\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "# 从llama_index库导入PyMuPDFReader类，用于读取和解析PDF文件内容\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "# 从llama_index库导入NodeWithScore和TextNode类\n",
    "# NodeWithScore: 表示带有相关性分数的节点，用于排序检索结果\n",
    "# TextNode: 表示文本块，是索引和检索的基本单位。节点存储文本内容及其元数据，便于构建知识图谱和语义搜索\n",
    "from llama_index.core.schema import NodeWithScore, TextNode\n",
    "# 从llama_index库导入RetrieverQueryEngine类，用于协调检索器和响应生成，执行端到端的问答过程\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "# 从llama_index库导入QueryBundle类，用于封装查询相关的信息，如查询文本、过滤器等\n",
    "from llama_index.core import QueryBundle\n",
    "# 从llama_index库导入BaseRetriever类，这是所有检索器的基类，定义了检索接口\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "# 从llama_index库导入SentenceSplitter类，用于将长文本分割成句子或语义完整的文本块，便于索引和检索\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "# 从llama_index库导入VectorStoreQuery类，用于构造向量存储的查询，支持语义相似度搜索\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "# 向量数据库\n",
    "import chromadb\n",
    "from ipex_llm.llamaindex.llms import IpexLLM\n",
    "\n",
    "class Config:\n",
    "    \"\"\"配置类,存储所有需要的参数\"\"\"\n",
    "    model_path = \"/home/qwen2chat_int4\"\n",
    "    tokenizer_path = \"/home/qwen2chat_int4\"\n",
    "    question = \"请你严格基于材料，告诉我qwen2的性能\"\n",
    "    data_path = \"/home/data/2407.10671v3.pdf\"\n",
    "    persist_dir = \"/home/chroma_db\"\n",
    "    embedding_model_path = \"/home/qwen2chat_src/AI-ModelScope/bge-small-zh-v1___5\"\n",
    "    max_new_tokens = 2048\n",
    "    if_persist_db = False # 决定是否要读取旧向量数据库，False 表示每次创建新向量数据库\n",
    "\n",
    "def load_vector_database(if_persist_db: bool,persist_dir: str) -> ChromaVectorStore:\n",
    "    \"\"\"\n",
    "    加载或创建向量数据库\n",
    "    \n",
    "    Args:\n",
    "        persist_dir (str): 持久化目录路径\n",
    "    \n",
    "    Returns:\n",
    "        ChromaVectorStore: 向量存储对象\n",
    "    \"\"\"\n",
    "    # 检查持久化目录是否存在\n",
    "    if if_persist_db:\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(f\"正在加载现有的向量数据库: {persist_dir}\")\n",
    "            chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "            chroma_collection = chroma_client.get_collection(\"qwen2_paper\")\n",
    "        else:\n",
    "            print(f\"创建新的向量数据库: {persist_dir}\")\n",
    "            chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "            chroma_collection = chroma_client.create_collection(\"qwen2_paper\")\n",
    "    else:\n",
    "        if os.path.exists(persist_dir):\n",
    "            print(f\"发现重复向量数据库，进行删除: {persist_dir}\")\n",
    "            shutil.rmtree(persist_dir)\n",
    "        print(f\"创建新的向量数据库: {persist_dir}\")\n",
    "        chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "        chroma_collection = chroma_client.create_collection(\"qwen2_paper\")\n",
    "    print(f\"Vector store loaded with {chroma_collection.count()} documents\")\n",
    "    return ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "def load_data(data_path: str) -> List[TextNode]:\n",
    "    \"\"\"\n",
    "    加载并处理PDF数据\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): PDF文件路径\n",
    "    \n",
    "    Returns:\n",
    "        List[TextNode]: 处理后的文本节点列表\n",
    "    \"\"\"\n",
    "    loader = PyMuPDFReader()\n",
    "    documents = loader.load(file_path=data_path)\n",
    "\n",
    "    text_parser = SentenceSplitter(chunk_size=2048)\n",
    "    text_chunks = []\n",
    "    doc_idxs = []\n",
    "    for doc_idx, doc in enumerate(documents):\n",
    "        cur_text_chunks = text_parser.split_text(doc.text)\n",
    "        text_chunks.extend(cur_text_chunks)\n",
    "        doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n",
    "    nodes = []\n",
    "    for idx, text_chunk in enumerate(text_chunks):\n",
    "        node = TextNode(text=text_chunk)\n",
    "        src_doc = documents[doc_idxs[idx]]\n",
    "        node.metadata = src_doc.metadata\n",
    "        nodes.append(node)\n",
    "    return nodes\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"向量数据库检索器\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: ChromaVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        检索相关文档\n",
    "        \n",
    "        Args:\n",
    "            query_bundle (QueryBundle): 查询包\n",
    "        \n",
    "        Returns:\n",
    "            List[NodeWithScore]: 检索到的文档节点及其相关性得分\n",
    "        \"\"\"\n",
    "        query_embedding = self._embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = self._vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "        print(f\"Retrieved {len(nodes_with_scores)} nodes with scores\")\n",
    "        return nodes_with_scores\n",
    "\n",
    "def completion_to_prompt(completion: str) -> str:\n",
    "    \"\"\"\n",
    "    将完成转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        completion (str): 完成的文本\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    return f\"<|system|>\\n</s>\\n<|user|>\\n{completion}</s>\\n<|assistant|>\\n\"\n",
    "\n",
    "def messages_to_prompt(messages: List[dict]) -> str:\n",
    "    \"\"\"\n",
    "    将消息列表转换为提示格式\n",
    "    \n",
    "    Args:\n",
    "        messages (List[dict]): 消息列表\n",
    "    \n",
    "    Returns:\n",
    "        str: 格式化后的提示\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == \"system\":\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"user\":\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == \"assistant\":\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def setup_llm(config: Config) -> IpexLLM:\n",
    "    \"\"\"\n",
    "    设置语言模型\n",
    "    \n",
    "    Args:\n",
    "        config (Config): 配置对象\n",
    "    \n",
    "    Returns:\n",
    "        IpexLLM: 配置好的语言模型\n",
    "    \"\"\"\n",
    "    return IpexLLM.from_model_id_low_bit(\n",
    "        model_name=config.model_path,\n",
    "        tokenizer_name=config.tokenizer_path,\n",
    "        context_window=4096,\n",
    "        max_new_tokens=config.max_new_tokens,\n",
    "        generate_kwargs={\"temperature\": 0.5, \"do_sample\": True},\n",
    "        model_kwargs={},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        completion_to_prompt=completion_to_prompt,\n",
    "        device_map=\"cpu\",\n",
    "    )\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    config = Config()\n",
    "    \n",
    "    # 设置嵌入模型\n",
    "    embed_model = HuggingFaceEmbedding(model_name=config.embedding_model_path)\n",
    "    \n",
    "    # 设置语言模型\n",
    "    llm = setup_llm(config)\n",
    "    \n",
    "    # 加载向量数据库\n",
    "    vector_store = load_vector_database(if_persist_db=config.if_persist_db,persist_dir=config.persist_dir)\n",
    "    \n",
    "    # 加载和处理数据\n",
    "    nodes = load_data(data_path=config.data_path)\n",
    "    for node in nodes:\n",
    "        node_embedding = embed_model.get_text_embedding(\n",
    "            node.get_content(metadata_mode=\"all\")\n",
    "        )\n",
    "        node.embedding = node_embedding\n",
    "    \n",
    "    # 将 node 添加到向量存储\n",
    "    vector_store.add(nodes)\n",
    "    \n",
    "    # 设置查询\n",
    "    query_str = config.question\n",
    "    query_embedding = embed_model.get_query_embedding(query_str)\n",
    "    \n",
    "    # 执行向量存储检索\n",
    "    print(\"开始执行向量存储检索\")\n",
    "    query_mode = \"default\"\n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    "    )\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # 处理查询结果\n",
    "    print(\"开始处理检索结果\")\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "    \n",
    "    # 设置检索器\n",
    "    retriever = VectorDBRetriever(\n",
    "        vector_store, embed_model, query_mode=\"default\", similarity_top_k=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Query engine created with retriever: {type(retriever).__name__}\")\n",
    "    print(f\"Query string length: {len(query_str)}\")\n",
    "    print(f\"Query string: {query_str}\")\n",
    "    \n",
    "    # 创建查询引擎\n",
    "    print(\"准备与llm对话\")\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)\n",
    "\n",
    "    # 执行查询\n",
    "    print(\"开始RAG最后生成\")\n",
    "    start_time = time.time()\n",
    "    response = query_engine.query(query_str)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"------------RESPONSE GENERATION---------------------\")\n",
    "    print(str(response))\n",
    "    print(f\"inference time: {time.time()-start_time}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当你能够看到正常的返回值,则证明你已经顺利实现了简单的 RAG 问答系统,接下来你可以尝试修改这个系统,或是用其他的 RAG 实现方法,来增强当前的问答体验.\n",
    "\n",
    "如果你已经通过了初赛，相信这些操作对你来说完全不难，大家可以尝试在当前机器上运行甚至20B以上参数量的模型；当然，最关键的还是内容是否具有创新性，但我们相信对你来说这些已不再是难题。\n",
    "\n",
    "祝你好运!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
